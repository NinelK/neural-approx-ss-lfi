{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect, time\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import discrepancy, visualization\n",
    "from algorithms import ABC_algorithms, TPABC, SMCABC, SMC2ABC, SNLABC, SNL2ABC\n",
    "import distributions \n",
    "import scipy.stats as stats\n",
    "\n",
    "import utils_os, utils_math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.ABC_problems import ABC_Problem\n",
    "\n",
    "class Neuronal_Problem(ABC_Problem):\n",
    "    \n",
    "    def __init__(self, data, N=100, n=100):\n",
    "        \n",
    "        assert N <= data['Y'].shape[0]\n",
    "        assert data['Y'].ndim == 2\n",
    "        assert data['X'].shape[0] == data['Y'].shape[0]\n",
    "        \n",
    "        self.N = N # number of posterior samples\n",
    "        self.n = n # length of the data vector x = {x_1, ..., x_n} # makes sense to make it ~num_trials\n",
    "#         self.d = 5 # this argument is ignored... set hidden_ratio instead. most likely it's dims of sufficient statistics: d=2K\n",
    "        self.prior_args = np.array([[0,1]]) # these are bounds on theta (on X in our case: [0,1])\n",
    "        \n",
    "        self.all_thetas = data['X']\n",
    "        self.sim_accuracy = 5 # number of digits after a decimal point for theta\n",
    "        self.sim = {np.round(data['X'][i],self.sim_accuracy): data['Y'][i] for i in range(data['X'].shape[0])} #here we use all!\n",
    "        self.K = 1 # number of thetas\n",
    "        self.stat = 'raw' # raw means that sufficient statistics is unknown (I guess). y_obs = data_obs\n",
    "        \n",
    "        self.data_obs = data['Y'] #important that first dim=N & y_dim = product of these dims\n",
    "        # y_obs is calculated from these data as y=statistics(data). \n",
    "        # note that y_obs is a argument of a Algorithm class, not Problem (¯\\_(ツ)_/¯)\n",
    "        \n",
    "        self.is_batch_sampling_supported = False # (unfinished feature, so keep False for now) speed up rejection sampling\n",
    "    \n",
    "    def get_true_theta(self):\n",
    "        pass # does not matter, as the result goes into 'statistics', where theta is currently not used\n",
    "\n",
    "    def sample_from_prior(self, size=1):\n",
    "        return np.random.choice(self.all_thetas,size=size,replace=True) # just 1 sample\n",
    "    \n",
    "    # original code samples only 1 theta in each simulation -> generates n x-es -> \n",
    "    # calculates statistics for them (1 vector for 1 theta) -> repeats ~1000 times sequentially (!)\n",
    "    def simulator(self, theta):\n",
    "        assert theta.size==1\n",
    "        y = np.empty((self.n,self.data_obs.shape[1])) \n",
    "        for i in range(self.n): \n",
    "            t = np.round(theta[0] + (np.random.rand()-0.5)*0.002,self.sim_accuracy) # add jitter, to sample from the neighbouring locations\n",
    "            if t in self.sim:\n",
    "                y[i] = self.sim[t]\n",
    "            else: # this part is used for newly-generated samples; let's take the Y=Y(closest X).\n",
    "                discr = np.abs(self.all_thetas - t) # get distances\n",
    "                y[i] = self.sim[np.round(self.all_thetas[np.argmin(discr)],self.sim_accuracy)] # take the closest\n",
    "        return y # self.n x number of dimensions in data\n",
    "\n",
    "    # B. correlation between latent\n",
    "    def _ss_corr(self, Z):\n",
    "        V = np.mat(Z).T * np.mat(Z) / Z.shape[0]\n",
    "        (d,d) = V.shape\n",
    "        upper_tri_elements = V[np.triu_indices(d, k=1)]\n",
    "        stat = np.array(upper_tri_elements)\n",
    "        return stat\n",
    "    \n",
    "    def statistics(self, data, theta=None):\n",
    "        if self.stat == 'raw':\n",
    "            # (correlation) as summary statistics (NO MARGINALS in these data)\n",
    "            stat = self._ss_corr(data)\n",
    "            return stat\n",
    "        else:\n",
    "            raise NotImplementedError('No ground truth statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(f'/home/nina/CopulaGP/plos_fig5_data/ST260_Day1_Dataset.pkl',\"rb\") as f:\n",
    "    data = pkl.load(f)\n",
    "    \n",
    "Nvar = 109 # taking the first N variables here\n",
    "data['Y'] = data['Y'][:,:Nvar] \n",
    "print(data['Y'].shape) # samples x neuronal/behavioral variables\n",
    "    \n",
    "problem = Neuronal_Problem(data)\n",
    "\n",
    "DIR = 'results/Neuronal' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sequential Neural Likelihood + \n",
    "hyperparams = ABC_algorithms.Hyperparams()\n",
    "hyperparams.save_dir = DIR\n",
    "hyperparams.device = 'cuda:0'\n",
    "hyperparams.num_sim = 1000                        # number of simulations\n",
    "hyperparams.L = 5                                # number of learning rounds\n",
    "hyperparams.hidden_ratio = 0.1                   # dimensionality of S(x)\n",
    "hyperparams.type = 'plain'                       # the network architecture of S(x), use CNN here\n",
    "hyperparams.estimator = 'DV'                    # MI estimator; JSD or DC, see the paper\n",
    "# 'DV' = proper MINE from Belghazi 2018\n",
    "hyperparams.nde = 'MAF'                          # nde; MAF (D>1) or MDN (D=1) # looks like D here is in fact K\n",
    "\n",
    "snl2_abc = SNL2ABC.SNL2_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snl2_abc.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check that the prior did not collapse \n",
    "theta = np.empty(1000)\n",
    "for i in range(len(theta)): \n",
    "    theta[i] = snl2_abc.prior()\n",
    "plt.xlim([0,1])\n",
    "plt.hist(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize latents s(x)\n",
    "nbins=100\n",
    "stats2plot = []\n",
    "for i in range(nbins):\n",
    "    mask = (data['X']>i/nbins) & (data['X']<=(i+1)/nbins)\n",
    "    get_stat = snl2_abc.convert_stat(snl2_abc.problem.statistics(data['Y'][mask]))\n",
    "    stats2plot.append(get_stat)\n",
    "# np.array(stats2plot).shape\n",
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(n_components=2).fit_transform(np.array(stats2plot).squeeze())\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "plt.scatter(*X_embedded.T,color=cm.rainbow(np.linspace(0,1,nbins)))\n",
    "plt.scatter(*X_embedded[int(nbins*60/160):int(nbins*120/160)].T,marker='x',color='k') #late part of the corridor marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MI using all generated subsamples\n",
    "all_stats = torch.tensor(np.vstack(snl2_abc.all_stats[0:snl2_abc.l+1])).float()\n",
    "all_samples = torch.tensor(np.vstack(snl2_abc.all_samples[0:snl2_abc.l+1])).float()\n",
    "print(all_samples.shape)\n",
    "snl2_abc.vae_net.MI(all_stats,all_samples,n=100) # n here is the number of shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MI using the last generated subsamples\n",
    "all_stats = torch.tensor(np.vstack(snl2_abc.all_stats[snl2_abc.l:snl2_abc.l+1])).float()\n",
    "all_samples = torch.tensor(np.vstack(snl2_abc.all_samples[snl2_abc.l:snl2_abc.l+1])).float()\n",
    "print(all_samples.shape)\n",
    "snl2_abc.vae_net.MI(all_stats,all_samples,n=100) # n here is the number of shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MI using newly picked samples \n",
    "# (here: statistics of variable size, but pooled from the fixed neighbourhood)\n",
    "new_stats = []\n",
    "new_samples = []\n",
    "for i in range(1000): # sample 1000 theta samples with replacement\n",
    "    theta = snl2_abc.problem.sample_from_prior()\n",
    "    mask = (data['X']>theta-1e-3) & ((data['X']<=theta+1e-3)) # we'll gather statistics from the neighbourhood of theta\n",
    "    stats = snl2_abc.problem.statistics(data['Y'][mask]) # the number of samples is variable here, but the size of the neighbourhood is fixed\n",
    "    samples = data['X'][mask].mean() # take mean theta from the neighbourhood (could as well just take theta)\n",
    "    new_stats.append(stats)\n",
    "    new_samples.append(samples) #theta \n",
    "new_stats = torch.tensor(new_stats).float().squeeze()\n",
    "new_samples = torch.tensor(new_samples).float().reshape((-1,1))\n",
    "print(new_stats.shape,new_samples.shape)\n",
    "snl2_abc.vae_net.MI(new_stats,new_samples,n=100) # n here is the number of shuffles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINE for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine import train_MINE # load another implementation, the one I used for PLoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MINE(data['Y'][:,:Nvar], x=torch.tensor(data['X'][:]).float(), \n",
    "           H=1000, lr=0.01, batches=1, n_epoch=2000, device = torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
