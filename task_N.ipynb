{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect, time\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import discrepancy, visualization\n",
    "from algorithms import ABC_algorithms, TPABC, SMCABC, SMC2ABC, SNLABC, SNL2ABC\n",
    "import distributions \n",
    "import scipy.stats as stats\n",
    "\n",
    "import utils_os, utils_math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.ABC_problems import ABC_Problem\n",
    "\n",
    "class Neuronal_Problem(ABC_Problem):\n",
    "    \n",
    "    def __init__(self, data, N=100, n=100):\n",
    "        \n",
    "        assert N <= data['Y'].shape[0]\n",
    "        assert data['Y'].ndim == 2\n",
    "        assert data['X'].shape[0] == data['Y'].shape[0]\n",
    "        \n",
    "        self.N = N # number of posterior samples\n",
    "        self.n = n # length of the data vector x = {x_1, ..., x_n} # makes sense to make it ~num_trials\n",
    "#         self.d = 5 # dims of sufficient statistics? d=2K? This argument is just not used anywhere... great\n",
    "        self.prior_args = np.array([[0,1]]) # these are bounds on theta (on X in our case: [0,1])\n",
    "        \n",
    "        self.all_thetas = data['X']\n",
    "        self.sim_accuracy = 5 # number of digits after a decimal point for theta\n",
    "        self.sim = {np.round(data['X'][i],self.sim_accuracy): data['Y'][i] for i in range(data['X'].shape[0])} #here we use all!\n",
    "        self.K = 1 # number of thetas\n",
    "        self.stat = 'raw' # raw means that sufficient statistics is unknown (I guess). y_obs = data_obs\n",
    "        \n",
    "        self.data_obs = data['Y'] #important that first dim=N & y_dim = product of these dims\n",
    "        # y_obs is calculated from these data as y=statistics(data). \n",
    "        # note that y_obs is a argument of a Algorithm class, not Problem (¯\\_(ツ)_/¯)\n",
    "        \n",
    "        self.is_batch_sampling_supported = False # (unfinished feature, so keep False for now) speed up rejection sampling\n",
    "    \n",
    "    def get_true_theta(self):\n",
    "        pass # does not matter, as the result goes into 'statistics', where theta is currently not used\n",
    "\n",
    "    def sample_from_prior(self, size=1):\n",
    "        return np.random.choice(self.all_thetas,size=size,replace=True) # just 1 sample\n",
    "    \n",
    "    # original code samples only 1 theta in each simulation -> generates n x-es -> \n",
    "    # calculates statistics for them (1 vector for 1 theta) -> repeats ~1000 times sequentially (!)\n",
    "    def simulator(self, theta):\n",
    "        assert theta.size==1\n",
    "        y = np.empty((self.n,self.data_obs.shape[1])) \n",
    "        for i in range(self.n): \n",
    "            t = np.round(theta[0] + (np.random.rand()-0.5)*0.01,self.sim_accuracy) # add jitter, to sample from the neighbouring locations\n",
    "            if t in self.sim:\n",
    "                y[i] = self.sim[t]\n",
    "            else: # this part is used for newly-generated samples; let's take the Y=Y(closest X).\n",
    "                discr = np.abs(self.all_thetas - t) # get distances\n",
    "                y[i] = self.sim[np.round(self.all_thetas[np.argmin(discr)],self.sim_accuracy)] # take the closest\n",
    "        return y # self.n x number of dimensions in data\n",
    "\n",
    "    # B. correlation between latent\n",
    "    def _ss_corr(self, Z):\n",
    "        V = np.mat(Z).T * np.mat(Z) / Z.shape[0]\n",
    "        (d,d) = V.shape\n",
    "        upper_tri_elements = V[np.triu_indices(d, k=1)]\n",
    "        stat = np.array(upper_tri_elements)\n",
    "        return stat\n",
    "    \n",
    "    def statistics(self, data, theta=None):\n",
    "        if self.stat == 'raw':\n",
    "            # (correlation) as summary statistics (NO MARGINALS in these data)\n",
    "            stat = self._ss_corr(data)\n",
    "            return stat\n",
    "        else:\n",
    "            raise NotImplementedError('No ground truth statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21471, 10)\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "with open(f'/home/nina/CopulaGP/plos_fig5_data/ST260_Day1_Dataset.pkl',\"rb\") as f:\n",
    "    data = pkl.load(f)\n",
    "    \n",
    "Nvar = 10 # taking the first N variables here\n",
    "data['Y'] = data['Y'][:,:Nvar] \n",
    "print(data['Y'].shape) # samples x neuronal/behavioral variables\n",
    "    \n",
    "problem = Neuronal_Problem(data)\n",
    "\n",
    "DIR = 'results/Neuronal' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sequential Neural Likelihood + \n",
    "hyperparams = ABC_algorithms.Hyperparams()\n",
    "hyperparams.save_dir = DIR\n",
    "hyperparams.device = 'cuda:0'\n",
    "hyperparams.num_sim = 1000                        # number of simulations\n",
    "hyperparams.L = 5                                # number of learning rounds\n",
    "hyperparams.hidden_ratio = 0.1                   # dimensionality of S(x)\n",
    "hyperparams.type = 'plain'                       # the network architecture of S(x), use CNN here\n",
    "hyperparams.estimator = 'DV'                    # MI estimator; JSD or DC, see the paper\n",
    "# 'DV' = proper MINE from Belghazi 2018\n",
    "hyperparams.nde = 'MAF'                          # nde; MAF (D>1) or MDN (D=1) # looks like D here is in fact K\n",
    "\n",
    "snl2_abc = SNL2ABC.SNL2_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  10\n",
      "[sampling] finished sampling  20\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  30\n",
      "[sampling] finished sampling  40\n",
      "[ABC] sub-process start!\n",
      "[ABC] sub-process start!\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 4 original dim = 45\n",
      "architecture [45, 100, 100, 4]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= 2.355128526687622e-05 loss val= 7.160007953643799e-06 time= 0.06008267402648926\n",
      "finished: t= 50 loss= -4.793889820575714e-05 loss val= -3.099068999290466e-05 time= 0.06429839134216309\n",
      "finished: t= 100 loss= -0.00011315383017063141 loss val= -6.365589797496796e-05 time= 0.06316351890563965\n",
      "finished: t= 150 loss= -0.00022386759519577026 loss val= -0.0001295078545808792 time= 0.06337785720825195\n",
      "finished: t= 200 loss= -0.00045620277523994446 loss val= -0.0003193523734807968 time= 0.06281161308288574\n",
      "finished: t= 250 loss= -0.0010997336357831955 loss val= -0.0007021557539701462 time= 0.06459736824035645\n",
      "finished: t= 300 loss= -0.002863280475139618 loss val= -0.0016330089420080185 time= 0.06571197509765625\n",
      "finished: t= 350 loss= -0.007558867335319519 loss val= -0.00487004779279232 time= 0.06261634826660156\n",
      "finished: t= 400 loss= -0.027072042226791382 loss val= -0.01786677911877632 time= 0.06252670288085938\n",
      "finished: t= 450 loss= -0.08283230662345886 loss val= -0.05717651546001434 time= 0.0626378059387207\n",
      "finished: t= 500 loss= -0.1312665343284607 loss val= -0.0901976004242897 time= 0.06499671936035156\n",
      "finished: t= 550 loss= -0.1561627984046936 loss val= -0.11903141438961029 time= 0.06362271308898926\n",
      "finished: t= 600 loss= -0.17684006690979004 loss val= -0.1314803659915924 time= 0.06222081184387207\n",
      "finished: t= 650 loss= -0.1949290782213211 loss val= -0.15536870062351227 time= 0.06321310997009277\n",
      "finished: t= 700 loss= -0.20879779756069183 loss val= -0.1666155457496643 time= 0.06384110450744629\n",
      "finished: t= 750 loss= -0.22685322165489197 loss val= -0.1914609670639038 time= 0.06332921981811523\n",
      "finished: t= 800 loss= -0.2423587590456009 loss val= -0.19159086048603058 time= 0.0614466667175293\n",
      "finished: t= 850 loss= -0.24936002492904663 loss val= -0.22091324627399445 time= 0.06241750717163086\n",
      "finished: t= 900 loss= -0.2723180949687958 loss val= -0.22306184470653534 time= 0.12017464637756348\n",
      "finished: t= 950 loss= -0.284254789352417 loss val= -0.2398758828639984 time= 0.07049965858459473\n",
      "finished: t= 1000 loss= -0.29757875204086304 loss val= -0.252296507358551 time= 0.07750940322875977\n",
      "finished: t= 1050 loss= -0.31544890999794006 loss val= -0.25264278054237366 time= 0.11614227294921875\n",
      "finished: t= 1100 loss= -0.33330151438713074 loss val= -0.2549991309642792 time= 0.0555419921875\n",
      "finished: t= 1150 loss= -0.34337109327316284 loss val= -0.2835438549518585 time= 0.05840921401977539\n",
      "finished: t= 1200 loss= -0.3562856614589691 loss val= -0.27889516949653625 time= 0.05540966987609863\n",
      "finished: t= 1250 loss= -0.3676425814628601 loss val= -0.28943926095962524 time= 0.053903818130493164\n",
      "finished: t= 1300 loss= -0.38584160804748535 loss val= -0.27232831716537476 time= 0.053507089614868164\n",
      "finished: t= 1350 loss= -0.3917413055896759 loss val= -0.2990551292896271 time= 0.05433225631713867\n",
      "finished: t= 1400 loss= -0.40996068716049194 loss val= -0.2872142493724823 time= 0.05935382843017578\n",
      "finished: t= 1450 loss= -0.4166461229324341 loss val= -0.30267995595932007 time= 0.05432605743408203\n",
      "best val loss= -0.31916043162345886\n",
      "\n",
      " > fitting nde\n",
      "all_stats.size() torch.Size([200, 4])\n",
      "finished: t= 0 loss= 4.279784202575684 loss val= 4.024518013000488\n",
      "best val loss= -7.574958801269531\n",
      "\n",
      " > fitting proposal\n",
      "mu= [[0.58518784]]\n",
      "cov= [[0.05549454]]\n",
      "\n",
      "\n",
      "iteration  1\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  10\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  20\n",
      "[sampling] finished sampling  30\n",
      "[sampling] finished sampling  40\n",
      "[ABC] sub-process start!\n",
      "[ABC] sub-process start!\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 4 original dim = 45\n",
      "architecture [45, 100, 100, 4]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= 3.7848949432373047e-06 loss val= -1.7583370208740234e-06 time= 0.13317441940307617\n",
      "finished: t= 50 loss= -2.384185791015625e-07 loss val= -1.3783574104309082e-07 time= 0.10610055923461914\n",
      "finished: t= 100 loss= -4.3001025915145874e-05 loss val= -3.394484519958496e-05 time= 0.1031026840209961\n",
      "finished: t= 150 loss= -0.00016976520419120789 loss val= -0.00015841424465179443 time= 0.10329079627990723\n",
      "finished: t= 200 loss= -0.0003750026226043701 loss val= -0.0003597438335418701 time= 0.10440444946289062\n",
      "finished: t= 250 loss= -0.0009942278265953064 loss val= -0.0009476728737354279 time= 0.10347843170166016\n",
      "finished: t= 300 loss= -0.0041723959147930145 loss val= -0.00425165519118309 time= 0.10345864295959473\n",
      "finished: t= 350 loss= -0.017916299402713776 loss val= -0.017717786133289337 time= 0.10199856758117676\n",
      "finished: t= 400 loss= -0.06573962420225143 loss val= -0.06675159186124802 time= 0.10212063789367676\n",
      "finished: t= 450 loss= -0.13936397433280945 loss val= -0.1457808017730713 time= 0.10198593139648438\n",
      "finished: t= 500 loss= -0.18302929401397705 loss val= -0.1915021538734436 time= 0.10235118865966797\n",
      "finished: t= 550 loss= -0.20819330215454102 loss val= -0.22574631869792938 time= 0.1022329330444336\n",
      "finished: t= 600 loss= -0.2341168373823166 loss val= -0.23908588290214539 time= 0.10196232795715332\n",
      "finished: t= 650 loss= -0.26141175627708435 loss val= -0.26601752638816833 time= 0.1019589900970459\n",
      "finished: t= 700 loss= -0.3080906271934509 loss val= -0.2987852990627289 time= 0.10182952880859375\n",
      "finished: t= 750 loss= -0.3392820358276367 loss val= -0.31775256991386414 time= 0.10213875770568848\n",
      "finished: t= 800 loss= -0.3665436804294586 loss val= -0.3267561197280884 time= 0.1025991439819336\n",
      "finished: t= 850 loss= -0.38641083240509033 loss val= -0.3441660702228546 time= 0.10288071632385254\n",
      "finished: t= 900 loss= -0.39906930923461914 loss val= -0.3563474416732788 time= 0.10205411911010742\n",
      "finished: t= 950 loss= -0.41331642866134644 loss val= -0.3731952905654907 time= 0.1019136905670166\n",
      "finished: t= 1000 loss= -0.42834264039993286 loss val= -0.38506072759628296 time= 0.10167098045349121\n",
      "finished: t= 1050 loss= -0.43874990940093994 loss val= -0.3951985239982605 time= 0.10195660591125488\n",
      "finished: t= 1100 loss= -0.4528604745864868 loss val= -0.40261465311050415 time= 0.10271334648132324\n",
      "finished: t= 1150 loss= -0.4552164673805237 loss val= -0.41141194105148315 time= 0.10217547416687012\n",
      "finished: t= 1200 loss= -0.4676317870616913 loss val= -0.42278480529785156 time= 0.10615825653076172\n",
      "finished: t= 1250 loss= -0.4760311245918274 loss val= -0.4269135594367981 time= 0.10270047187805176\n",
      "finished: t= 1300 loss= -0.4891529977321625 loss val= -0.4226706922054291 time= 0.10225987434387207\n",
      "finished: t= 1350 loss= -0.48857659101486206 loss val= -0.4409242272377014 time= 0.10228800773620605\n",
      "finished: t= 1400 loss= -0.49339982867240906 loss val= -0.45160427689552307 time= 0.12486815452575684\n",
      "finished: t= 1450 loss= -0.4948144555091858 loss val= -0.44070205092430115 time= 0.11898398399353027\n",
      "finished: t= 1500 loss= -0.5067964792251587 loss val= -0.44479668140411377 time= 0.11811280250549316\n",
      "finished: t= 1550 loss= -0.5053786635398865 loss val= -0.45650815963745117 time= 0.11668038368225098\n",
      "finished: t= 1600 loss= -0.5111898183822632 loss val= -0.46779417991638184 time= 0.1173865795135498\n",
      "finished: t= 1650 loss= -0.5173135995864868 loss val= -0.47191017866134644 time= 0.11780595779418945\n",
      "finished: t= 1700 loss= -0.5246188640594482 loss val= -0.4710119962692261 time= 0.11676883697509766\n",
      "finished: t= 1750 loss= -0.5282185077667236 loss val= -0.46892407536506653 time= 0.1167597770690918\n",
      "finished: t= 1800 loss= -0.5374345183372498 loss val= -0.48635363578796387 time= 0.11701655387878418\n",
      "finished: t= 1850 loss= -0.5393107533454895 loss val= -0.4787431061267853 time= 0.11762857437133789\n",
      "finished: t= 1900 loss= -0.5396169424057007 loss val= -0.4895994961261749 time= 0.11814761161804199\n",
      "finished: t= 1950 loss= -0.5472863912582397 loss val= -0.4849992096424103 time= 0.1174626350402832\n",
      "finished: t= 2000 loss= -0.5536447167396545 loss val= -0.48544618487358093 time= 0.10284757614135742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: t= 2050 loss= -0.5566120147705078 loss val= -0.48240596055984497 time= 0.10230898857116699\n",
      "finished: t= 2100 loss= -0.5611029267311096 loss val= -0.48560208082199097 time= 0.10588335990905762\n",
      "finished: t= 2150 loss= -0.5689806938171387 loss val= -0.4888806641101837 time= 0.10399627685546875\n",
      "finished: t= 2200 loss= -0.5713651180267334 loss val= -0.5013905167579651 time= 0.10353422164916992\n",
      "finished: t= 2250 loss= -0.5737749338150024 loss val= -0.49033012986183167 time= 0.1038355827331543\n",
      "finished: t= 2300 loss= -0.5885589718818665 loss val= -0.4954584836959839 time= 0.1035609245300293\n",
      "finished: t= 2350 loss= -0.5811280012130737 loss val= -0.5000746846199036 time= 0.1008749008178711\n",
      "finished: t= 2400 loss= -0.5959227681159973 loss val= -0.4994288980960846 time= 0.10441875457763672\n",
      "finished: t= 2450 loss= -0.5967461466789246 loss val= -0.5066975951194763 time= 0.10544919967651367\n",
      "finished: t= 2500 loss= -0.5968956351280212 loss val= -0.5021742582321167 time= 0.10324311256408691\n",
      "finished: t= 2550 loss= -0.5989401936531067 loss val= -0.5026577115058899 time= 0.10336470603942871\n",
      "finished: t= 2600 loss= -0.6078150272369385 loss val= -0.5003893375396729 time= 0.10278916358947754\n",
      "finished: t= 2650 loss= -0.6068769693374634 loss val= -0.4885617792606354 time= 0.10258054733276367\n",
      "finished: t= 2700 loss= -0.6148005723953247 loss val= -0.49849021434783936 time= 0.10326409339904785\n",
      "finished: t= 2750 loss= -0.6111132502555847 loss val= -0.49929484724998474 time= 0.10312747955322266\n",
      "finished: t= 2800 loss= -0.6211535930633545 loss val= -0.5023735761642456 time= 0.10338783264160156\n",
      "best val loss= -0.5180580615997314\n",
      "\n",
      " > fitting nde\n",
      "all_stats.size() torch.Size([400, 4])\n",
      "finished: t= 0 loss= 4.136393070220947 loss val= 3.9566123485565186\n",
      "best val loss= -8.196948051452637\n",
      "\n",
      " > fitting proposal\n",
      "mu= [[0.64416314]]\n",
      "cov= [[0.04599001]]\n",
      "\n",
      "\n",
      "iteration  2\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  10\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  20\n",
      "[sampling] finished sampling  30\n",
      "[sampling] finished sampling  40\n",
      "[ABC] sub-process start!\n",
      "[ABC] sub-process start!\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 4 original dim = 45\n",
      "architecture [45, 100, 100, 4]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= 8.717179298400879e-07 loss val= 3.7997961044311523e-07 time= 0.0822303295135498\n",
      "finished: t= 50 loss= -6.400048732757568e-06 loss val= -3.978610038757324e-06 time= 0.07906913757324219\n",
      "finished: t= 100 loss= -0.00042795389890670776 loss val= -0.00033691897988319397 time= 0.07889175415039062\n",
      "finished: t= 150 loss= -0.005231127142906189 loss val= -0.005397532135248184 time= 0.07711803913116455\n",
      "finished: t= 200 loss= -0.06512313336133957 loss val= -0.057831596583127975 time= 0.07765138149261475\n",
      "finished: t= 250 loss= -0.15562085807323456 loss val= -0.13029003143310547 time= 0.07759642601013184\n",
      "finished: t= 300 loss= -0.2030029296875 loss val= -0.1360720843076706 time= 0.07878673076629639\n",
      "finished: t= 350 loss= -0.16992038488388062 loss val= -0.15457822382450104 time= 0.07981455326080322\n",
      "finished: t= 400 loss= -0.2220456898212433 loss val= -0.15719132125377655 time= 0.08154439926147461\n",
      "finished: t= 450 loss= -0.23751208186149597 loss val= -0.17213238775730133 time= 0.08945202827453613\n",
      "finished: t= 500 loss= -0.2782798409461975 loss val= -0.17168141901493073 time= 0.12506914138793945\n",
      "finished: t= 550 loss= -0.3034244179725647 loss val= -0.17823998630046844 time= 0.1506873369216919\n",
      "finished: t= 600 loss= -0.31170326471328735 loss val= -0.19325946271419525 time= 0.11162829399108887\n",
      "finished: t= 650 loss= -0.2818118929862976 loss val= -0.18465235829353333 time= 0.10604071617126465\n",
      "finished: t= 700 loss= -0.30395224690437317 loss val= -0.1972070187330246 time= 0.10129892826080322\n",
      "finished: t= 750 loss= -0.3613978326320648 loss val= -0.20784257352352142 time= 0.07567501068115234\n",
      "finished: t= 800 loss= -0.3456346392631531 loss val= -0.21579669415950775 time= 0.08470714092254639\n",
      "finished: t= 850 loss= -0.3685801029205322 loss val= -0.22882993519306183 time= 0.08066451549530029\n",
      "finished: t= 900 loss= -0.3904820680618286 loss val= -0.2237946093082428 time= 0.07759201526641846\n",
      "finished: t= 950 loss= -0.3869335651397705 loss val= -0.24310681223869324 time= 0.07786083221435547\n",
      "finished: t= 1000 loss= -0.3759780526161194 loss val= -0.24030238389968872 time= 0.07672595977783203\n",
      "finished: t= 1050 loss= -0.39765292406082153 loss val= -0.26274967193603516 time= 0.08773493766784668\n",
      "finished: t= 1100 loss= -0.42544180154800415 loss val= -0.2643870413303375 time= 0.09542131423950195\n",
      "finished: t= 1150 loss= -0.3921533226966858 loss val= -0.27628013491630554 time= 0.08915853500366211\n",
      "finished: t= 1200 loss= -0.45787325501441956 loss val= -0.27212199568748474 time= 0.07956206798553467\n",
      "finished: t= 1250 loss= -0.4712139964103699 loss val= -0.2933080196380615 time= 0.07983875274658203\n",
      "finished: t= 1300 loss= -0.4781235456466675 loss val= -0.2914624512195587 time= 0.08067142963409424\n",
      "finished: t= 1350 loss= -0.4540539085865021 loss val= -0.2998793125152588 time= 0.07776772975921631\n",
      "finished: t= 1400 loss= -0.4546675980091095 loss val= -0.30917420983314514 time= 0.07889032363891602\n",
      "finished: t= 1450 loss= -0.44814956188201904 loss val= -0.30542775988578796 time= 0.07847857475280762\n",
      "finished: t= 1500 loss= -0.49944940209388733 loss val= -0.3157358765602112 time= 0.07950067520141602\n",
      "finished: t= 1550 loss= -0.45590049028396606 loss val= -0.32533928751945496 time= 0.07833874225616455\n",
      "finished: t= 1600 loss= -0.5127996802330017 loss val= -0.32871392369270325 time= 0.07807409763336182\n",
      "finished: t= 1650 loss= -0.47123846411705017 loss val= -0.3230324685573578 time= 0.07697045803070068\n",
      "finished: t= 1700 loss= -0.49298930168151855 loss val= -0.3334311842918396 time= 0.0779564380645752\n",
      "finished: t= 1750 loss= -0.5310940742492676 loss val= -0.333354115486145 time= 0.07753539085388184\n",
      "finished: t= 1800 loss= -0.5430766344070435 loss val= -0.3391392230987549 time= 0.07806551456451416\n",
      "finished: t= 1850 loss= -0.4641062915325165 loss val= -0.33563005924224854 time= 0.0906914472579956\n",
      "finished: t= 1900 loss= -0.5506253242492676 loss val= -0.3337011933326721 time= 0.09072864055633545\n",
      "finished: t= 1950 loss= -0.47648245096206665 loss val= -0.32331275939941406 time= 0.08901941776275635\n",
      "finished: t= 2000 loss= -0.571041464805603 loss val= -0.33094269037246704 time= 0.08975350856781006\n",
      "finished: t= 2050 loss= -0.5774569511413574 loss val= -0.3378000855445862 time= 0.12867867946624756\n",
      "finished: t= 2100 loss= -0.5950042009353638 loss val= -0.34338030219078064 time= 0.12865638732910156\n",
      "finished: t= 2150 loss= -0.5351375937461853 loss val= -0.33931082487106323 time= 0.12865984439849854\n",
      "finished: t= 2200 loss= -0.5505445599555969 loss val= -0.34888941049575806 time= 0.089455246925354\n",
      "finished: t= 2250 loss= -0.503156304359436 loss val= -0.3488827049732208 time= 0.07566869258880615\n",
      "finished: t= 2300 loss= -0.5571050643920898 loss val= -0.3529663681983948 time= 0.07976138591766357\n",
      "finished: t= 2350 loss= -0.5795453786849976 loss val= -0.3537139296531677 time= 0.08007156848907471\n",
      "finished: t= 2400 loss= -0.5553669929504395 loss val= -0.35560286045074463 time= 0.07880604267120361\n",
      "finished: t= 2450 loss= -0.647826075553894 loss val= -0.3630036413669586 time= 0.07984042167663574\n",
      "finished: t= 2500 loss= -0.6008577942848206 loss val= -0.36866068840026855 time= 0.08003079891204834\n",
      "finished: t= 2550 loss= -0.5923327207565308 loss val= -0.35204607248306274 time= 0.07941257953643799\n",
      "finished: t= 2600 loss= -0.5959290266036987 loss val= -0.36115747690200806 time= 0.08011317253112793\n",
      "finished: t= 2650 loss= -0.5529006719589233 loss val= -0.36614543199539185 time= 0.07764244079589844\n",
      "finished: t= 2700 loss= -0.6060858368873596 loss val= -0.3597412109375 time= 0.0802379846572876\n",
      "finished: t= 2750 loss= -0.6117174625396729 loss val= -0.35773685574531555 time= 0.07860183715820312\n",
      "finished: t= 2800 loss= -0.5616445541381836 loss val= -0.36882483959198 time= 0.08006978034973145\n",
      "finished: t= 2850 loss= -0.5777424573898315 loss val= -0.35537537932395935 time= 0.07923829555511475\n",
      "finished: t= 2900 loss= -0.6027466654777527 loss val= -0.3469237685203552 time= 0.0797419548034668\n",
      "finished: t= 2950 loss= -0.5873652100563049 loss val= -0.3605819046497345 time= 0.0778425931930542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best val loss= -0.37690871953964233\n",
      "\n",
      " > fitting nde\n",
      "all_stats.size() torch.Size([600, 4])\n",
      "finished: t= 0 loss= 4.303411960601807 loss val= 4.137106895446777\n",
      "best val loss= -8.844435691833496\n",
      "\n",
      " > fitting proposal\n",
      "mu= [[0.72572182]]\n",
      "cov= [[0.03691813]]\n",
      "\n",
      "\n",
      "iteration  3\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  10\n",
      "[sampling] finished sampling  20\n",
      "[sampling] finished sampling  30\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  40\n",
      "[ABC] sub-process start!\n",
      "[ABC] sub-process start!\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 4 original dim = 45\n",
      "architecture [45, 100, 100, 4]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= -2.4922192096710205e-06 loss val= -9.685754776000977e-06 time= 0.08285315831502278\n",
      "finished: t= 50 loss= -0.0004916787147521973 loss val= -0.00044563040137290955 time= 0.07020028432210286\n",
      "finished: t= 100 loss= -0.003825962543487549 loss val= -0.0036005377769470215 time= 0.06931233406066895\n",
      "finished: t= 150 loss= -0.04923224449157715 loss val= -0.05252731591463089 time= 0.07258248329162598\n",
      "finished: t= 200 loss= -0.17154744267463684 loss val= -0.14562872052192688 time= 0.06995876630147298\n",
      "finished: t= 250 loss= -0.18418478965759277 loss val= -0.17029792070388794 time= 0.07038116455078125\n",
      "finished: t= 300 loss= -0.2507080137729645 loss val= -0.19073742628097534 time= 0.07244602839152019\n",
      "finished: t= 350 loss= -0.30115145444869995 loss val= -0.20973315834999084 time= 0.07355388005574544\n",
      "finished: t= 400 loss= -0.27970701456069946 loss val= -0.26423054933547974 time= 0.07125218709309895\n",
      "finished: t= 450 loss= -0.3066557049751282 loss val= -0.2894335389137268 time= 0.07291396458943684\n",
      "finished: t= 500 loss= -0.3701933026313782 loss val= -0.3199845850467682 time= 0.0688016414642334\n",
      "finished: t= 550 loss= -0.31540101766586304 loss val= -0.3356917202472687 time= 0.07213298479715984\n",
      "finished: t= 600 loss= -0.3189184069633484 loss val= -0.35519036650657654 time= 0.0731964111328125\n",
      "finished: t= 650 loss= -0.40144941210746765 loss val= -0.3779408931732178 time= 0.07168134053548177\n",
      "finished: t= 700 loss= -0.3893170654773712 loss val= -0.4064987301826477 time= 0.07623020807902019\n",
      "finished: t= 750 loss= -0.3681049942970276 loss val= -0.4035781919956207 time= 0.07932122548421223\n",
      "finished: t= 800 loss= -0.447565495967865 loss val= -0.43019068241119385 time= 0.07808891932169597\n",
      "finished: t= 850 loss= -0.43882089853286743 loss val= -0.4491914212703705 time= 0.06957840919494629\n",
      "finished: t= 900 loss= -0.44918447732925415 loss val= -0.4603150188922882 time= 0.06867678960164388\n",
      "finished: t= 950 loss= -0.5007312297821045 loss val= -0.4605863690376282 time= 0.06842827796936035\n",
      "finished: t= 1000 loss= -0.4459381699562073 loss val= -0.47766172885894775 time= 0.0682363510131836\n",
      "finished: t= 1050 loss= -0.3829249441623688 loss val= -0.48938727378845215 time= 0.07062935829162598\n",
      "finished: t= 1100 loss= -0.47355571389198303 loss val= -0.4904654324054718 time= 0.07032084465026855\n",
      "finished: t= 1150 loss= -0.3694615662097931 loss val= -0.506140947341919 time= 0.0690019925435384\n",
      "finished: t= 1200 loss= -0.5562835931777954 loss val= -0.4981899857521057 time= 0.07510987917582194\n",
      "finished: t= 1250 loss= -0.47411906719207764 loss val= -0.5080603957176208 time= 0.07331093152364095\n",
      "finished: t= 1300 loss= -0.48068875074386597 loss val= -0.5045162439346313 time= 0.07515549659729004\n"
     ]
    }
   ],
   "source": [
    "snl2_abc.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = snl2_abc.problem.sample_from_prior(size=20000)\n",
    "# net = snl2_abc.nde_net\n",
    "# y_obs, theta = snl2_abc.convert_stat(snl2_abc.whiten(snl2_abc.y_obs)), theta\n",
    "# # y_obs = np.repeat(y_obs,400,axis=0)\n",
    "# print(y_obs.shape)\n",
    "# y_obs, theta = torch.tensor(y_obs).float(), torch.tensor(theta).float().view(1, -1)\n",
    "# log_probs = net.log_probs(inputs=y_obs, cond_inputs=theta)\n",
    "snl2_abc.problem.data_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check that the prior did not collapse \n",
    "theta = np.empty(1000)\n",
    "for i in range(len(theta)): \n",
    "    theta[i] = snl2_abc.prior()\n",
    "plt.xlim([0,1])\n",
    "plt.hist(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31910545, 0.31400834, 0.31528498, 0.31293586, 0.3094438 ,\n",
       "        0.30839044, 0.30834512, 0.30658993, 0.30696861, 0.31580506,\n",
       "        0.31325759, 0.31492224, 0.30785461, 0.30674789, 0.29722566,\n",
       "        0.30548434, 0.29056185, 0.30904688, 0.31521884, 0.31955345,\n",
       "        0.31123648, 0.29890438, 0.32111345, 0.28171838, 0.30982338,\n",
       "        0.30561098, 0.30197579, 0.29698256, 0.30139333, 0.28665786,\n",
       "        0.30832488, 0.30771774, 0.29709593, 0.30674394, 0.28172374,\n",
       "        0.30755499, 0.2958246 , 0.31737984, 0.27935311, 0.29927007,\n",
       "        0.30416235, 0.27669616, 0.29417504, 0.28358472, 0.27666595]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get latents s(x)\n",
    "snl2_abc.convert_stat(snl2_abc.problem.statistics(data['Y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MI using all generated subsamples\n",
    "all_stats = torch.tensor(np.vstack(snl2_abc.all_stats[0:snl2_abc.l+1])).float()\n",
    "all_samples = torch.tensor(np.vstack(snl2_abc.all_samples[0:snl2_abc.l+1])).float()\n",
    "print(all_samples.shape)\n",
    "snl2_abc.vae_net.MI(all_stats,all_samples,n=100) # n here is the number of shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MI using the last generated subsamples\n",
    "all_stats = torch.tensor(np.vstack(snl2_abc.all_stats[snl2_abc.l:snl2_abc.l+1])).float()\n",
    "all_samples = torch.tensor(np.vstack(snl2_abc.all_samples[snl2_abc.l:snl2_abc.l+1])).float()\n",
    "print(all_samples.shape)\n",
    "snl2_abc.vae_net.MI(all_stats,all_samples,n=100) # n here is the number of shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MI using newly picked samples \n",
    "# (here: statistics of variable size, but pooled from the fixed neighbourhood)\n",
    "new_stats = []\n",
    "new_samples = []\n",
    "for i in range(1000): # sample 1000 theta samples with replacement\n",
    "    theta = snl2_abc.problem.sample_from_prior()\n",
    "    mask = (data['X']>theta-1e-3) & ((data['X']<=theta+1e-3)) # we'll gather statistics from the neighbourhood of theta\n",
    "    stats = snl2_abc.problem.statistics(data['Y'][mask]) # the number of samples is variable here, but the size of the neighbourhood is fixed\n",
    "    samples = data['X'][mask].mean() # take mean theta from the neighbourhood (could as well just take theta)\n",
    "    new_stats.append(stats)\n",
    "    new_samples.append(samples) #theta \n",
    "new_stats = torch.tensor(new_stats).float().squeeze()\n",
    "new_samples = torch.tensor(new_samples).float().reshape((-1,1))\n",
    "print(new_stats.shape,new_samples.shape)\n",
    "snl2_abc.vae_net.MI(new_stats,new_samples,n=100) # n here is the number of shuffles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINE for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mine import train_MINE # load another implementation, the one I used for PLoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_MINE(data['Y'][:,:Nvar], x=torch.tensor(data['X'][:]).float(), \n",
    "           H=1000, lr=0.01, batches=1, n_epoch=2000, device = torch.device(\"cuda:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
