{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, inspect, time\n",
    "\n",
    "import numpy as np\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "import discrepancy, visualization\n",
    "from algorithms import ABC_algorithms, TPABC, SMCABC, SMC2ABC, SNLABC, SNL2ABC\n",
    "import distributions \n",
    "import scipy.stats as stats\n",
    "\n",
    "import utils_os, utils_math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.ABC_problems import ABC_Problem\n",
    "\n",
    "class Gauss_Problem(ABC_Problem): #problem from Fig.4A in PLoS CB Copula paper\n",
    "    \n",
    "    def __init__(self, N=100, n=100):\n",
    "        \n",
    "        self.N = N # number of posterior samples\n",
    "        self.n = n # length of the data vector x = {x_1, ..., x_n}\n",
    "        self.D = 5 # dimensionality of data samples\n",
    "#         self.d = 5 # dims of sufficient statistics? d=2K? This argument is just not used anywhere... great\n",
    "        self.prior_args = np.array([[0,1]]) # these are bounds on theta (on X in our case: [0,1])\n",
    "        \n",
    "        self.K = 1 # number of thetas\n",
    "        self.stat = 'raw' # raw means that sufficient statistics is unknown (I guess). y_obs = data_obs\n",
    "        \n",
    "#         self.data_obs = self.y_obs #important that first dim=N & y_dim = product of these dims\n",
    "        \n",
    "    def get_true_theta(self):\n",
    "        pass # does not matter, as the result goes into 'statistics', where theta is currently not used\n",
    "\n",
    "    def sample_from_prior(self,size=1):\n",
    "        return np.random.rand(size)\n",
    "    \n",
    "    def simulator(self, theta):\n",
    "        rho = -0.1 + (0.999+0.1)*theta #problem from Fig.4A in PLoS CB Copula paper, linearly changing rho\n",
    "        samples = np.zeros((theta.size,self.n,self.D))\n",
    "        for i,r in enumerate(rho):\n",
    "            samples[i] = np.random.multivariate_normal(np.zeros(self.D),np.ones((self.D,self.D))*r +\\\n",
    "                                                np.eye(self.D)*(1-r),size=self.n)\n",
    "        return samples.reshape((-1,self.D)) # [number of theta samples (normally 1) x self.n] x self.D\n",
    "\n",
    "    # B. correlation between latent\n",
    "    def _ss_corr(self, Z):\n",
    "        V = np.mat(Z).T * np.mat(Z) / Z.shape[0]\n",
    "        (d,d) = V.shape\n",
    "        upper_tri_elements = V[np.triu_indices(d, k=1)]\n",
    "        stat = np.array(upper_tri_elements)\n",
    "        return stat\n",
    "    \n",
    "    def statistics(self, data, theta=None):\n",
    "        if self.stat == 'raw':\n",
    "            # (correlation) as summary statistics (all margianls are N(0,1) here, don't matter)\n",
    "            stat = self._ss_corr(data)\n",
    "            return stat\n",
    "        else:\n",
    "            raise NotImplementedError('No ground truth statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'results/Gauss'\n",
    "\n",
    "problem = Gauss_Problem()\n",
    "theta = problem.sample_from_prior(size=100) # sample some thetas\n",
    "problem.data_obs = problem.simulator(theta) # generate 'true' observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sequential Neural Likelihood + \n",
    "hyperparams = ABC_algorithms.Hyperparams()\n",
    "hyperparams.save_dir = DIR\n",
    "hyperparams.device = 'cuda:0'\n",
    "hyperparams.num_sim = 1000                        # number of simulations\n",
    "hyperparams.L = 5                                # number of learning rounds\n",
    "hyperparams.hidden_ratio = 0.1                   # dimensionality of S(x)\n",
    "hyperparams.type = 'plain'                       # the network architecture of S(x), use CNN here\n",
    "hyperparams.estimator = 'NWJ'                    # MI estimator; JSD or DC, see the paper\n",
    "hyperparams.nde = 'MDN'                          # nde; MAF (D>1) or MDN (D=1)\n",
    "\n",
    "snl2_abc = SNL2ABC.SNL2_ABC(problem, discrepancy=discrepancy.eculidean_dist, hyperparams=hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start![ABC] sub-process start!\n",
      "[ABC] sub-process start!\n",
      "[ABC] sub-process start!\n",
      "\n",
      "[sampling] finished sampling  10\n",
      "[sampling] finished sampling  20\n",
      "[sampling] finished sampling  30\n",
      "[sampling] finished sampling  40\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 1 original dim = 10\n",
      "architecture [10, 100, 100, 1]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= 0.5862871408462524 loss val= 0.5824804306030273 time= 0.0644533634185791\n",
      "finished: t= 50 loss= 0.38697391748428345 loss val= 0.3889622390270233 time= 0.06012606620788574\n",
      "finished: t= 100 loss= 0.13830256462097168 loss val= 0.14646920561790466 time= 0.05761003494262695\n",
      "finished: t= 150 loss= 0.02726912498474121 loss val= 0.03884536027908325 time= 0.060192108154296875\n",
      "finished: t= 200 loss= 0.006212353706359863 loss val= 0.009167909622192383 time= 0.054904937744140625\n",
      "finished: t= 250 loss= 0.0016643404960632324 loss val= 0.0025238394737243652 time= 0.05496382713317871\n",
      "finished: t= 300 loss= -0.0020328164100646973 loss val= -0.0030347704887390137 time= 0.0550532341003418\n",
      "finished: t= 350 loss= -0.012072265148162842 loss val= -0.01826190948486328 time= 0.05707263946533203\n",
      "finished: t= 400 loss= -0.028281033039093018 loss val= -0.0420154333114624 time= 0.06211733818054199\n",
      "finished: t= 450 loss= -0.0618748664855957 loss val= -0.08955228328704834 time= 0.05563020706176758\n",
      "finished: t= 500 loss= -0.14191186428070068 loss val= -0.2047339677810669 time= 0.060063838958740234\n",
      "finished: t= 550 loss= -0.21649456024169922 loss val= -0.27503883838653564 time= 0.060752153396606445\n",
      "finished: t= 600 loss= -0.3252214193344116 loss val= -0.34692084789276123 time= 0.05737113952636719\n",
      "finished: t= 650 loss= -0.41995322704315186 loss val= -0.41389918327331543 time= 0.05620145797729492\n",
      "finished: t= 700 loss= -0.5252866148948669 loss val= -0.4629242420196533 time= 0.05296444892883301\n",
      "finished: t= 750 loss= -0.6223606467247009 loss val= -0.5638009309768677 time= 0.05571150779724121\n",
      "finished: t= 800 loss= -0.7316555380821228 loss val= -0.6169034242630005 time= 0.05581235885620117\n",
      "finished: t= 850 loss= -0.8165318965911865 loss val= -0.678564190864563 time= 0.05447244644165039\n",
      "finished: t= 900 loss= -0.8962317109107971 loss val= -0.7991294860839844 time= 0.05659985542297363\n",
      "finished: t= 950 loss= -0.9481573700904846 loss val= -0.8735065460205078 time= 0.05603361129760742\n",
      "finished: t= 1000 loss= -1.006160020828247 loss val= -0.9611726999282837 time= 0.0554499626159668\n",
      "finished: t= 1050 loss= -1.0570622682571411 loss val= -0.9831526279449463 time= 0.0576019287109375\n",
      "finished: t= 1100 loss= -1.0964179039001465 loss val= -1.0261690616607666 time= 0.055856943130493164\n",
      "finished: t= 1150 loss= -1.1299470663070679 loss val= -1.1184813976287842 time= 0.05546855926513672\n",
      "finished: t= 1200 loss= -1.1412303447723389 loss val= -1.0971487760543823 time= 0.05525636672973633\n",
      "finished: t= 1250 loss= -1.1805979013442993 loss val= -1.14619779586792 time= 0.054532766342163086\n",
      "finished: t= 1300 loss= -1.1840274333953857 loss val= -1.1601567268371582 time= 0.05348348617553711\n",
      "finished: t= 1350 loss= -1.191184163093567 loss val= -1.1329309940338135 time= 0.053713321685791016\n",
      "finished: t= 1400 loss= -1.2038946151733398 loss val= -1.136252760887146 time= 0.05595254898071289\n",
      "finished: t= 1450 loss= -1.2222100496292114 loss val= -1.1838051080703735 time= 0.054148197174072266\n",
      "finished: t= 1500 loss= -1.2076910734176636 loss val= -1.2193564176559448 time= 0.05350852012634277\n",
      "finished: t= 1550 loss= -1.2215769290924072 loss val= -1.1453834772109985 time= 0.055655479431152344\n",
      "finished: t= 1600 loss= -1.24729585647583 loss val= -1.2058634757995605 time= 0.056473493576049805\n",
      "finished: t= 1650 loss= -1.2422354221343994 loss val= -1.212708830833435 time= 0.055327653884887695\n",
      "finished: t= 1700 loss= -1.2474617958068848 loss val= -1.1903067827224731 time= 0.05635881423950195\n",
      "finished: t= 1750 loss= -1.2605805397033691 loss val= -1.2364652156829834 time= 0.055557966232299805\n",
      "best val loss= -1.2905102968215942\n",
      "\n",
      " > fitting nde\n",
      "all_stats.size() torch.Size([200, 1])\n",
      "finished: t= 0 loss= 21.412771224975586 loss val= 23.983749389648438\n",
      "finished: t= 250 loss= 4.01984167098999 loss val= 4.101640224456787\n",
      "finished: t= 500 loss= 3.7449631690979004 loss val= 3.8000831604003906\n",
      "finished: t= 750 loss= 3.1963069438934326 loss val= 3.2812912464141846\n",
      "finished: t= 1000 loss= 2.559826612472534 loss val= 2.843867063522339\n",
      "finished: t= 1250 loss= 2.2097327709198 loss val= 2.471600294113159\n",
      "finished: t= 1500 loss= 2.150892734527588 loss val= 2.457237482070923\n",
      "best val loss= 2.4539220333099365\n",
      "\n",
      "\n",
      "iteration  1\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start!\n",
      "[ABC] sub-process start![sampling] finished sampling \n",
      "[ABC] sub-process start! 10\n",
      "[ABC] sub-process start!\n",
      "\n",
      "[sampling] finished sampling  20\n",
      "[sampling] finished sampling  30\n",
      "[sampling] finished sampling  40\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 1 original dim = 10\n",
      "architecture [10, 100, 100, 1]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= 0.46620625257492065 loss val= 0.45862510800361633 time= 0.12317848205566406\n",
      "finished: t= 50 loss= 0.1690516173839569 loss val= 0.16497021913528442 time= 0.10543370246887207\n",
      "finished: t= 100 loss= 0.004002273082733154 loss val= 0.004685461521148682 time= 0.10561680793762207\n",
      "finished: t= 150 loss= -0.0001698136329650879 loss val= -4.798173904418945e-05 time= 0.10599994659423828\n",
      "finished: t= 200 loss= -0.001249551773071289 loss val= -0.0013914704322814941 time= 0.12556147575378418\n",
      "finished: t= 250 loss= -0.002657651901245117 loss val= -0.002853989601135254 time= 0.11163091659545898\n",
      "finished: t= 300 loss= -0.0050154924392700195 loss val= -0.005337119102478027 time= 0.10618758201599121\n",
      "finished: t= 350 loss= -0.009290635585784912 loss val= -0.009902775287628174 time= 0.10557818412780762\n",
      "finished: t= 400 loss= -0.015661239624023438 loss val= -0.01643359661102295 time= 0.10363125801086426\n",
      "finished: t= 450 loss= -0.02708280086517334 loss val= -0.028566360473632812 time= 0.10670042037963867\n",
      "finished: t= 500 loss= -0.05070585012435913 loss val= -0.05434536933898926 time= 0.10486912727355957\n",
      "finished: t= 550 loss= -0.10436558723449707 loss val= -0.11149191856384277 time= 0.10687851905822754\n",
      "finished: t= 600 loss= -0.21998858451843262 loss val= -0.2379658818244934 time= 0.10423135757446289\n",
      "finished: t= 650 loss= -0.40131425857543945 loss val= -0.428503155708313 time= 0.10341501235961914\n",
      "finished: t= 700 loss= -0.618794858455658 loss val= -0.6516571640968323 time= 0.10759687423706055\n",
      "finished: t= 750 loss= -0.8163965344429016 loss val= -0.8462615609169006 time= 0.10556721687316895\n",
      "finished: t= 800 loss= -0.9797989726066589 loss val= -1.004066824913025 time= 0.10588550567626953\n",
      "finished: t= 850 loss= -1.087278127670288 loss val= -1.1200686693191528 time= 0.10481786727905273\n",
      "finished: t= 900 loss= -1.1537411212921143 loss val= -1.1773730516433716 time= 0.10563993453979492\n",
      "finished: t= 950 loss= -1.2018407583236694 loss val= -1.2513641119003296 time= 0.10341691970825195\n",
      "finished: t= 1000 loss= -1.2518755197525024 loss val= -1.268343448638916 time= 0.10258603096008301\n",
      "finished: t= 1050 loss= -1.2728773355484009 loss val= -1.320594072341919 time= 0.1054220199584961\n",
      "finished: t= 1100 loss= -1.2862738370895386 loss val= -1.2991803884506226 time= 0.10598945617675781\n",
      "finished: t= 1150 loss= -1.2943933010101318 loss val= -1.3065396547317505 time= 0.10549449920654297\n",
      "finished: t= 1200 loss= -1.308110237121582 loss val= -1.316138505935669 time= 0.1060037612915039\n",
      "finished: t= 1250 loss= -1.316890001296997 loss val= -1.3138474225997925 time= 0.10528731346130371\n",
      "finished: t= 1300 loss= -1.3275798559188843 loss val= -1.3177406787872314 time= 0.10753679275512695\n",
      "finished: t= 1350 loss= -1.3357610702514648 loss val= -1.3366568088531494 time= 0.10899949073791504\n",
      "finished: t= 1400 loss= -1.3430018424987793 loss val= -1.3390488624572754 time= 0.10355997085571289\n",
      "finished: t= 1450 loss= -1.3598237037658691 loss val= -1.3249120712280273 time= 0.1026759147644043\n",
      "finished: t= 1500 loss= -1.3565129041671753 loss val= -1.341333270072937 time= 0.10402178764343262\n",
      "finished: t= 1550 loss= -1.363374948501587 loss val= -1.3411779403686523 time= 0.10623812675476074\n",
      "finished: t= 1600 loss= -1.3730005025863647 loss val= -1.3474912643432617 time= 0.1082451343536377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: t= 1650 loss= -1.3668513298034668 loss val= -1.3363839387893677 time= 0.10549592971801758\n",
      "finished: t= 1700 loss= -1.3824882507324219 loss val= -1.3583734035491943 time= 0.10770988464355469\n",
      "finished: t= 1750 loss= -1.3706225156784058 loss val= -1.3514580726623535 time= 0.10589408874511719\n",
      "finished: t= 1800 loss= -1.3881570100784302 loss val= -1.3427209854125977 time= 0.10665249824523926\n",
      "finished: t= 1850 loss= -1.3822171688079834 loss val= -1.3541969060897827 time= 0.10704851150512695\n",
      "finished: t= 1900 loss= -1.3913023471832275 loss val= -1.3572800159454346 time= 0.10420584678649902\n",
      "best val loss= -1.384033441543579\n",
      "\n",
      " > fitting nde\n",
      "all_stats.size() torch.Size([400, 1])\n",
      "finished: t= 0 loss= 25.443403244018555 loss val= 25.521623611450195\n",
      "finished: t= 250 loss= 3.996159315109253 loss val= 4.017160892486572\n",
      "finished: t= 500 loss= 3.2251229286193848 loss val= 3.2285313606262207\n",
      "finished: t= 750 loss= 1.851866602897644 loss val= 1.8917019367218018\n",
      "best val loss= 1.8880970478057861\n",
      "\n",
      "\n",
      "iteration  2\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start!\n",
      "[ABC] sub-process start![ABC] sub-process start!\n",
      "\n",
      "[sampling] finished sampling [ABC] sub-process start! \n",
      "10\n",
      "[sampling] finished sampling  20\n",
      "[sampling] finished sampling  30\n",
      "[sampling] finished sampling  40\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 1 original dim = 10\n",
      "architecture [10, 100, 100, 1]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= 0.5204662084579468 loss val= 0.514074444770813 time= 0.08543002605438232\n",
      "finished: t= 50 loss= 0.022627711296081543 loss val= 0.020456552505493164 time= 0.07758784294128418\n",
      "finished: t= 100 loss= 0.0019268393516540527 loss val= 0.0018621087074279785 time= 0.08003270626068115\n",
      "finished: t= 150 loss= -0.0038614273071289062 loss val= -0.004310905933380127 time= 0.07657361030578613\n",
      "finished: t= 200 loss= -0.013940811157226562 loss val= -0.014573633670806885 time= 0.07656025886535645\n",
      "finished: t= 250 loss= -0.06710696220397949 loss val= -0.0700337290763855 time= 0.07770061492919922\n",
      "finished: t= 300 loss= -0.3708934783935547 loss val= -0.4013553261756897 time= 0.07549774646759033\n",
      "finished: t= 350 loss= -0.7872474789619446 loss val= -0.8712647557258606 time= 0.07707405090332031\n",
      "finished: t= 400 loss= -1.020064353942871 loss val= -1.0829838514328003 time= 0.08138167858123779\n",
      "finished: t= 450 loss= -1.129341721534729 loss val= -1.1981149911880493 time= 0.07953059673309326\n",
      "finished: t= 500 loss= -1.2232933044433594 loss val= -1.2762095928192139 time= 0.07801878452301025\n",
      "finished: t= 550 loss= -1.2970319986343384 loss val= -1.300767421722412 time= 0.07637441158294678\n",
      "finished: t= 600 loss= -1.2516698837280273 loss val= -1.3197505474090576 time= 0.07603371143341064\n",
      "finished: t= 650 loss= -1.288459062576294 loss val= -1.3402715921401978 time= 0.08171737194061279\n",
      "finished: t= 700 loss= -1.3260040283203125 loss val= -1.340311884880066 time= 0.07802867889404297\n",
      "finished: t= 750 loss= -1.2780225276947021 loss val= -1.352250099182129 time= 0.0820239782333374\n",
      "finished: t= 800 loss= -1.2714189291000366 loss val= -1.343255639076233 time= 0.07757115364074707\n",
      "finished: t= 850 loss= -1.3646165132522583 loss val= -1.3532575368881226 time= 0.10790562629699707\n",
      "finished: t= 900 loss= -1.3700153827667236 loss val= -1.3489433526992798 time= 0.08233439922332764\n",
      "finished: t= 950 loss= -1.355748176574707 loss val= -1.3470523357391357 time= 0.08361601829528809\n",
      "finished: t= 1000 loss= -1.3648006916046143 loss val= -1.3652637004852295 time= 0.07806074619293213\n",
      "finished: t= 1050 loss= -1.3219248056411743 loss val= -1.3453187942504883 time= 0.07825672626495361\n",
      "finished: t= 1100 loss= -1.3198715448379517 loss val= -1.3494150638580322 time= 0.07843327522277832\n",
      "finished: t= 1150 loss= -1.3357325792312622 loss val= -1.3673373460769653 time= 0.07791423797607422\n",
      "finished: t= 1200 loss= -1.331264615058899 loss val= -1.3568352460861206 time= 0.07902681827545166\n",
      "finished: t= 1250 loss= -1.4096558094024658 loss val= -1.359841227531433 time= 0.07663559913635254\n",
      "finished: t= 1300 loss= -1.350858449935913 loss val= -1.3614345788955688 time= 0.0776824951171875\n",
      "finished: t= 1350 loss= -1.3868730068206787 loss val= -1.3666096925735474 time= 0.07770919799804688\n",
      "finished: t= 1400 loss= -1.3371003866195679 loss val= -1.386209487915039 time= 0.07848107814788818\n",
      "finished: t= 1450 loss= -1.39235258102417 loss val= -1.374714970588684 time= 0.07679665088653564\n",
      "finished: t= 1500 loss= -1.3894048929214478 loss val= -1.354003667831421 time= 0.07732689380645752\n",
      "finished: t= 1550 loss= -1.3919415473937988 loss val= -1.3779997825622559 time= 0.07665562629699707\n",
      "finished: t= 1600 loss= -1.3938734531402588 loss val= -1.3839558362960815 time= 0.07620096206665039\n",
      "finished: t= 1650 loss= -1.3413259983062744 loss val= -1.3647912740707397 time= 0.07716786861419678\n",
      "finished: t= 1700 loss= -1.3804240226745605 loss val= -1.3630564212799072 time= 0.07879030704498291\n",
      "finished: t= 1750 loss= -1.345456600189209 loss val= -1.384276032447815 time= 0.07788193225860596\n",
      "finished: t= 1800 loss= -1.3795467615127563 loss val= -1.3757089376449585 time= 0.08217167854309082\n",
      "finished: t= 1850 loss= -1.4081493616104126 loss val= -1.3959970474243164 time= 0.07837188243865967\n",
      "finished: t= 1900 loss= -1.3694206476211548 loss val= -1.391857385635376 time= 0.07991480827331543\n",
      "finished: t= 1950 loss= -1.3482611179351807 loss val= -1.3670140504837036 time= 0.07900631427764893\n",
      "finished: t= 2000 loss= -1.3555474281311035 loss val= -1.393821120262146 time= 0.07803213596343994\n",
      "finished: t= 2050 loss= -1.344223141670227 loss val= -1.3886289596557617 time= 0.07517397403717041\n",
      "finished: t= 2100 loss= -1.3965997695922852 loss val= -1.3784605264663696 time= 0.07856249809265137\n",
      "finished: t= 2150 loss= -1.3458342552185059 loss val= -1.3838272094726562 time= 0.07509219646453857\n",
      "finished: t= 2200 loss= -1.3402570486068726 loss val= -1.3975588083267212 time= 0.08060991764068604\n",
      "finished: t= 2250 loss= -1.4532243013381958 loss val= -1.399821162223816 time= 0.07862472534179688\n",
      "finished: t= 2300 loss= -1.4188711643218994 loss val= -1.3785483837127686 time= 0.07745957374572754\n",
      "finished: t= 2350 loss= -1.3475778102874756 loss val= -1.3936973810195923 time= 0.07652795314788818\n",
      "finished: t= 2400 loss= -1.4041962623596191 loss val= -1.3926572799682617 time= 0.07838094234466553\n",
      "finished: t= 2450 loss= -1.40569007396698 loss val= -1.4198756217956543 time= 0.07991468906402588\n",
      "best val loss= -1.420691728591919\n",
      "\n",
      " > fitting nde\n",
      "all_stats.size() torch.Size([600, 1])\n",
      "finished: t= 0 loss= 26.35706329345703 loss val= 26.14179229736328\n",
      "finished: t= 250 loss= 4.142261028289795 loss val= 4.116366863250732\n",
      "finished: t= 500 loss= 3.036930561065674 loss val= 3.0271222591400146\n",
      "finished: t= 750 loss= 1.881778359413147 loss val= 1.8933106660842896\n",
      "best val loss= 1.8750413656234741\n",
      "\n",
      "\n",
      "iteration  3\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start![ABC] sub-process start![ABC] sub-process start![ABC] sub-process start!\n",
      "\n",
      "\n",
      "\n",
      "[sampling] finished sampling  10\n",
      "[sampling] finished sampling  20\n",
      "[sampling] finished sampling  30\n",
      "[sampling] finished sampling  40\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 1 original dim = 10\n",
      "architecture [10, 100, 100, 1]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= 0.2801438868045807 loss val= 0.27343180775642395 time= 0.07962576548258464\n",
      "finished: t= 50 loss= -0.000973045825958252 loss val= -0.0008970499038696289 time= 0.07139698664347331\n",
      "finished: t= 100 loss= -0.010373473167419434 loss val= -0.010548233985900879 time= 0.07139404614766438\n",
      "finished: t= 150 loss= -0.18436622619628906 loss val= -0.1745002269744873 time= 0.0727229118347168\n",
      "finished: t= 200 loss= -0.5862475037574768 loss val= -0.5593563318252563 time= 0.07044704755147298\n",
      "finished: t= 250 loss= -0.8368397951126099 loss val= -0.8291065096855164 time= 0.06981213887532552\n",
      "finished: t= 300 loss= -1.041374683380127 loss val= -1.0294983386993408 time= 0.07194638252258301\n",
      "finished: t= 350 loss= -1.1396697759628296 loss val= -1.134110689163208 time= 0.0716567834218343\n",
      "finished: t= 400 loss= -1.178957462310791 loss val= -1.2130458354949951 time= 0.07158724466959636\n",
      "finished: t= 450 loss= -1.3289496898651123 loss val= -1.2454495429992676 time= 0.07100001970926921\n",
      "finished: t= 500 loss= -1.2749135494232178 loss val= -1.267486572265625 time= 0.0714403788248698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished: t= 550 loss= -1.2558083534240723 loss val= -1.273091435432434 time= 0.07226800918579102\n",
      "finished: t= 600 loss= -1.3558943271636963 loss val= -1.2791733741760254 time= 0.0718849500020345\n",
      "finished: t= 650 loss= -1.355332374572754 loss val= -1.2851152420043945 time= 0.07134326299031575\n",
      "finished: t= 700 loss= -1.3218410015106201 loss val= -1.2853959798812866 time= 0.07122548421223958\n",
      "finished: t= 750 loss= -1.3398327827453613 loss val= -1.294788122177124 time= 0.0873250166575114\n",
      "finished: t= 800 loss= -1.2976503372192383 loss val= -1.2920335531234741 time= 0.06926528612772624\n",
      "finished: t= 850 loss= -1.3911255598068237 loss val= -1.3067947626113892 time= 0.07001868883768718\n",
      "finished: t= 900 loss= -1.3813962936401367 loss val= -1.3068760633468628 time= 0.07134747505187988\n",
      "finished: t= 950 loss= -1.3453569412231445 loss val= -1.3141629695892334 time= 0.06995320320129395\n",
      "finished: t= 1000 loss= -1.4685027599334717 loss val= -1.310555100440979 time= 0.06998761494954427\n",
      "finished: t= 1050 loss= -1.3627303838729858 loss val= -1.3096089363098145 time= 0.06910181045532227\n",
      "finished: t= 1100 loss= -1.453350305557251 loss val= -1.298269271850586 time= 0.070770263671875\n",
      "finished: t= 1150 loss= -1.4084711074829102 loss val= -1.3111417293548584 time= 0.07304906845092773\n",
      "finished: t= 1200 loss= -1.293662667274475 loss val= -1.3197330236434937 time= 0.07024423281351726\n",
      "finished: t= 1250 loss= -1.3965864181518555 loss val= -1.3221676349639893 time= 0.07016134262084961\n",
      "finished: t= 1300 loss= -1.4129056930541992 loss val= -1.2831568717956543 time= 0.06956617037455241\n",
      "finished: t= 1350 loss= -1.3801456689834595 loss val= -1.3088127374649048 time= 0.0703426996866862\n",
      "finished: t= 1400 loss= -1.4784729480743408 loss val= -1.3178778886795044 time= 0.07109991709391277\n",
      "best val loss= -1.3435544967651367\n",
      "\n",
      " > fitting nde\n",
      "all_stats.size() torch.Size([800, 1])\n",
      "finished: t= 0 loss= 18.013952255249023 loss val= 17.45587730407715\n",
      "finished: t= 250 loss= 3.5056092739105225 loss val= 3.4455478191375732\n",
      "finished: t= 500 loss= 3.2525718212127686 loss val= 3.1990394592285156\n",
      "finished: t= 750 loss= 2.2885518074035645 loss val= 2.262037754058838\n",
      "finished: t= 1000 loss= 1.9617795944213867 loss val= 1.9819080829620361\n",
      "finished: t= 1250 loss= 1.889715552330017 loss val= 1.9264042377471924\n",
      "best val loss= 1.9233381748199463\n",
      "\n",
      "\n",
      "iteration  4\n",
      "# of cpus =  4\n",
      "[ABC] sub-process start![ABC] sub-process start!\n",
      "\n",
      "[ABC] sub-process start!\n",
      "[sampling] finished sampling  [ABC] sub-process start!10\n",
      "\n",
      "[sampling] finished sampling  20\n",
      "[sampling] finished sampling  30\n",
      "[sampling] finished sampling  40\n",
      "\n",
      " > fitting encoder\n",
      "summary statistic dim = 1 original dim = 10\n",
      "architecture [10, 100, 100, 1]\n",
      "validation size= 0.8\n",
      "finished: t= 0 loss= 0.1721743643283844 loss val= 0.16728048026561737 time= 0.06918090581893921\n",
      "finished: t= 50 loss= -0.012862682342529297 loss val= -0.013712108135223389 time= 0.06542366743087769\n",
      "finished: t= 100 loss= -0.5348578095436096 loss val= -0.5387901067733765 time= 0.0648687481880188\n",
      "finished: t= 150 loss= -1.121020793914795 loss val= -1.0718088150024414 time= 0.06773686408996582\n",
      "finished: t= 200 loss= -1.1940687894821167 loss val= -1.1759006977081299 time= 0.06524038314819336\n",
      "finished: t= 250 loss= -1.3045613765716553 loss val= -1.2227859497070312 time= 0.06596595048904419\n",
      "finished: t= 300 loss= -1.2780942916870117 loss val= -1.2310338020324707 time= 0.06713229417800903\n",
      "finished: t= 350 loss= -1.3710103034973145 loss val= -1.2522783279418945 time= 0.07448947429656982\n",
      "finished: t= 400 loss= -1.3474018573760986 loss val= -1.2684509754180908 time= 0.06360572576522827\n",
      "finished: t= 450 loss= -1.3044795989990234 loss val= -1.2882258892059326 time= 0.06501209735870361\n",
      "finished: t= 500 loss= -1.3650591373443604 loss val= -1.2729743719100952 time= 0.0645034909248352\n",
      "finished: t= 550 loss= -1.3875389099121094 loss val= -1.2982702255249023 time= 0.0648530125617981\n",
      "finished: t= 600 loss= -1.3207834959030151 loss val= -1.270383358001709 time= 0.06482726335525513\n",
      "finished: t= 650 loss= -1.4520364999771118 loss val= -1.2959654331207275 time= 0.0654492974281311\n",
      "finished: t= 700 loss= -1.386405348777771 loss val= -1.2933318614959717 time= 0.06485939025878906\n",
      "finished: t= 750 loss= -1.3286352157592773 loss val= -1.3031914234161377 time= 0.06541991233825684\n",
      "finished: t= 800 loss= -1.3794628381729126 loss val= -1.2903947830200195 time= 0.06494992971420288\n",
      "finished: t= 850 loss= -1.3873776197433472 loss val= -1.2858225107192993 time= 0.06431823968887329\n",
      "finished: t= 900 loss= -1.524948239326477 loss val= -1.3047940731048584 time= 0.06450760364532471\n",
      "finished: t= 950 loss= -1.4033564329147339 loss val= -1.3267486095428467 time= 0.06457042694091797\n",
      "finished: t= 1000 loss= -1.347346305847168 loss val= -1.3107805252075195 time= 0.06533533334732056\n",
      "finished: t= 1050 loss= -1.3732707500457764 loss val= -1.314834713935852 time= 0.06518179178237915\n",
      "best val loss= -1.3311035633087158\n",
      "\n",
      " > fitting nde\n",
      "all_stats.size() torch.Size([1000, 1])\n",
      "finished: t= 0 loss= 21.014711380004883 loss val= 19.67725372314453\n",
      "finished: t= 250 loss= 3.672659158706665 loss val= 3.6225457191467285\n",
      "finished: t= 500 loss= 2.710789680480957 loss val= 2.7620513439178467\n",
      "finished: t= 750 loss= 2.191471815109253 loss val= 2.216273069381714\n",
      "finished: t= 1000 loss= 2.102952480316162 loss val= 2.0874557495117188\n",
      "finished: t= 1250 loss= 2.075793504714966 loss val= 2.0424411296844482\n",
      "best val loss= 2.031721353530884\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snl2_abc.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([109.,  95., 116., 104.,  96.,  93.,  79., 116.,  89., 103.]),\n",
       " array([1.62951717e-06, 9.98927073e-02, 1.99783785e-01, 2.99674863e-01,\n",
       "        3.99565940e-01, 4.99457018e-01, 5.99348096e-01, 6.99239174e-01,\n",
       "        7.99130251e-01, 8.99021329e-01, 9.98912407e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1d08744e0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let us check that the prior did not collapse \n",
    "theta = np.empty(1000)\n",
    "for i in range(len(theta)): \n",
    "    theta[i] = snl2_abc.prior()\n",
    "plt.xlim([0,1])\n",
    "plt.hist(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate MI using all generated subsamples\n",
    "all_stats = torch.tensor(np.vstack(snl2_abc.all_stats[0:snl2_abc.l+1])).float()\n",
    "all_samples = torch.tensor(np.vstack(snl2_abc.all_samples[0:snl2_abc.l+1])).float()\n",
    "print(all_samples.shape)\n",
    "snl2_abc.vae_net.MI(all_stats,all_samples,n=100) # n here is the number of shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.4432, grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate MI using the last generated subsamples\n",
    "all_stats = torch.tensor(np.vstack(snl2_abc.all_stats[snl2_abc.l:snl2_abc.l+1])).float()\n",
    "all_samples = torch.tensor(np.vstack(snl2_abc.all_samples[snl2_abc.l:snl2_abc.l+1])).float()\n",
    "print(all_samples.shape)\n",
    "snl2_abc.vae_net.MI(all_stats,all_samples,n=100) # n here is the number of shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Fig.4A in PLoS CB paper for correct answers. Should be roughly 1 bit for 10 dims (D), 0.7 bits for 5 dims\n",
    "# DV looks alright! \n",
    "# DC just always returns 0.9something, whatever the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cb04b29cf14e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0msummary_PCA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary2plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0msummary_TSNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary2plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0msummary_LEM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpectralEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary2plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd1d0817828>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize latents s(x)\n",
    "nbins=100\n",
    "stats2plot = []\n",
    "data2plot = []\n",
    "summary2plot = []\n",
    "for i in range(nbins):\n",
    "    theta = np.repeat(i/(nbins-1),100) #100 simulations\n",
    "    generated = snl2_abc.problem.simulator(theta)\n",
    "    get_stat = snl2_abc.convert_stat(snl2_abc.problem.statistics(generated))\n",
    "    stats2plot.append(get_stat)\n",
    "    data2plot.append(generated.mean(0))\n",
    "    summary2plot.append(snl2_abc.problem.statistics(generated))\n",
    "    \n",
    "plt.plot(np.array(stats2plot).squeeze())\n",
    "# np.array(stats2plot).shape\n",
    "# from sklearn.manifold import TSNE, SpectralEmbedding\n",
    "# from sklearn.decomposition import PCA\n",
    "# latent_PCA = PCA(n_components=2).fit_transform(np.array(stats2plot).reshape(-1,1))\n",
    "# latent_TSNE = TSNE(n_components=2).fit_transform(np.array(stats2plot).squeeze())\n",
    "# latent_LEM = SpectralEmbedding(n_components=2).fit_transform(np.array(stats2plot).squeeze())\n",
    "\n",
    "\n",
    "summary_PCA = PCA(n_components=2).fit_transform(np.array(summary2plot).squeeze())\n",
    "summary_TSNE = TSNE(n_components=2).fit_transform(np.array(summary2plot).squeeze())\n",
    "summary_LEM = SpectralEmbedding(n_components=2).fit_transform(np.array(summary2plot).squeeze())\n",
    "\n",
    "\n",
    "data_PCA = PCA(n_components=2).fit_transform(np.array(data2plot).squeeze())\n",
    "data_TSNE = TSNE(n_components=2).fit_transform(np.array(data2plot).squeeze())\n",
    "data_LEM = SpectralEmbedding(n_components=2).fit_transform(np.array(data2plot).squeeze())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "fig = plt.figure(figsize=(7,6))\n",
    "\n",
    "# fig.add_axes([0,0,1,1])\n",
    "\n",
    "\n",
    "ax = [fig.add_axes([0.05,0,0.2,0.25]),\n",
    "      fig.add_axes([0.3,0,0.2,0.25]),\n",
    "      fig.add_axes([0.55,0,0.2,0.25]),\n",
    "     fig.add_axes([0.05,0.35,0.2,0.25]),\n",
    "     fig.add_axes([0.3,0.35,0.2,0.25]),\n",
    "      fig.add_axes([0.55,0.35,0.2,0.25]),\n",
    "     fig.add_axes([0.05,0.7,0.2,0.25]),\n",
    "     fig.add_axes([0.3,0.7,0.2,0.25]),\n",
    "      fig.add_axes([0.55,0.7,0.2,0.25]),\n",
    "     fig.add_axes([0.85,0.3,0.1,0.4]),]\n",
    "\n",
    "ax[0].scatter(*data_PCA.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "ax[1].scatter(*data_TSNE.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "ax[2].scatter(*data_LEM.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "\n",
    "ax[3].scatter(*summary_PCA.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "ax[4].scatter(*summary_TSNE.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "ax[5].scatter(*summary_LEM.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "\n",
    "# ax[6].scatter(*latent_PCA.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "# ax[7].scatter(*latent_TSNE.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "# ax[8].scatter(*latent_LEM.T,color=cm.rainbow(np.linspace(0,1,nbins)),s=5)\n",
    "\n",
    "for a in [ax[0],ax[3],ax[6]]:\n",
    "    a.set_xlabel('PCA 1')\n",
    "    a.set_ylabel('PCA 2')\n",
    "    \n",
    "for a in [ax[1],ax[4],ax[7]]:\n",
    "    a.set_xlabel('t-SNE 1')\n",
    "    a.set_ylabel('t-SNE 2')\n",
    "    \n",
    "for a in [ax[2],ax[5],ax[8]]:\n",
    "    a.set_xlabel('L-eigen 1')\n",
    "    a.set_ylabel('L-eigen 2')\n",
    "    \n",
    "for a in ax[:-1]:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "    \n",
    "ax[9].imshow(np.repeat(np.linspace(0,1,160).reshape(1,-1),10,axis=0).T,cmap=cm.rainbow)\n",
    "ax[9].set_ylabel(\"Position in VR, cm\")\n",
    "\n",
    "fig.text(0.27,0.96,'Sufficient statistics',fontsize=14)\n",
    "fig.text(0.27,0.61,'Summary statistics',fontsize=14)\n",
    "fig.text(0.32,0.26,'Binned data',fontsize=14)\n",
    "\n",
    "# help(plt.imshow)\n",
    "# plt.scatter(*X_embedded[int(nbins*20/160):int(nbins*60/160)].T,marker='x',color='k') #late part of the corridor marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
